{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a text classification model, we need to feed the entire sentence (or the entire news article) into a neural network. The problem here is that each article/sentence has a variable length; and all convolutional or fully connected neural networks handle a fixed input size. There are two ways we can handle this problem:\n",
    "\n",
    "Find a way to collapse a sentence into fixed-length TF-IDF and bag-of-words vector representations.\n",
    "\n",
    "Design special neural network architectures that can handle variable length sequences \n",
    "\n",
    "Recurrent Neural Networks (RNN).\n",
    "\n",
    "Bag of Words\n",
    "Vector representation is the most widely used traditional vector representation. Each word is linked to a vector index, the vector element contains the number of occurrences of a word in a given document.\n",
    "\n",
    "In the BoW representation, word occurrences are weighted evenly, regardless of the word itself. However, it is clear that frequent words, such as \"a\", \"in\", \"the\", etc., are much less important for classification than specialized terms. In fact, in most NLP tasks some words are more relevant than others. Then TF-IDF is the best approach.\n",
    "\n",
    "TF-IDF\n",
    "It means term frequency: inverse document frequency. It is a variation of bag of words, where instead of a binary 0/1 value indicating the occurrence of a word in a document, a floating point value is used, which is related to the frequency of occurrence of words in the corpus .\n",
    "\n",
    "The formula to calculate TF-IDF is: w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
    "\n",
    "Here is the meaning of each parameter in the formula:\n",
    "\n",
    "I is the word\n",
    "\n",
    "j is the document\n",
    "\n",
    "w_{ij} is the weight or importance of the word in the document\n",
    "\n",
    "tf_{ij} is the number of occurrences of the word $i$ in the document $j$, that is, the BoW value that we have seen before\n",
    "\n",
    "N is the number of documents in the collection.\n",
    "\n",
    "df_i is the number of documents containing the word $i$ in the entire collection\n",
    "\n",
    "The TF-IDF value w_{ij} increases proportionally to the number of times a word appears in a document and is compensated by the number of documents in the corpus that contain the word, which helps adjust for the fact that some words appear more frequently. What others. For example, if the word appears in all documents in the collection, df_i=N and w_{ij}=0, those terms would be completely ignored.\n",
    "\n",
    "\n",
    "\n",
    "Although the TF-IDF representation calculates different weights for different words based on their importance, it cannot correctly capture meaning, largely because the order of words in the sentence is still not taken into account."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
